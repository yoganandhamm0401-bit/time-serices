# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/190gzzicQ7AHHQVfv6iU3gJGx4YkhDNYM
"""

import numpy as np
import pandas as pd
import argparse

def generate_multivariate(n_steps=2000, n_series=4, seed=42):
    np.random.seed(seed)
    t = np.arange(n_steps)
    df = pd.DataFrame({'t': t})
    # global trend
    trend = 0.0005 * (t**1.5)
    for i in range(n_series):
        # seasonalities
        p1 = 24 + i*2  # daily-ish
        p2 = 168 + i*5 # weekly-ish
        s1 = 2.0 * np.sin(2*np.pi * t / p1 + np.random.randn()*0.1)
        s2 = 1.5 * np.sin(2*np.pi * t / p2 + np.random.randn()*0.1)
        # regime: introduce a change halfway
        regime = np.where(t > n_steps//2, 1.0 + 0.3*i, 1.0)
        # interacting component (a linear combo of previous series and noise)
        inter = 0
        if i > 0:
            inter = 0.25 * (np.sin(2*np.pi * t / (20 + i)) + np.cos(2*np.pi * t / (50+i)))
        noise = 0.5 * np.random.normal(size=n_steps)
        base = (trend * (1 + 0.05*i) + s1 + 0.6*s2) * regime + inter + noise
        df[f'x{i}'] = base
    # target is a weighted sum with a lag and small noise
    # create a target 'y' that depends on x0..xn with lags
    y = np.zeros(n_steps)
    for i in range(n_series):
        lag = 1 + (i % 3)
        shifted = np.roll(df[f'x{i}'].values, lag)
        # add small nonlinearity
        y += 0.4 * (np.tanh(0.01 * shifted) * (1 + 0.1*np.sin(t/50.0) * (i+1)))
    y += 0.3 * np.random.normal(size=n_steps)
    df['y'] = y
    return df

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--out', '--out_file', dest='out', default='experiments/data.csv')
    parser.add_argument('--n_steps', type=int, default=2000)
    parser.add_argument('--n_series', type=int, default=4)
    args = parser.parse_args()
    df = generate_multivariate(n_steps=args.n_steps, n_series=args.n_series)
    # Save
    df.to_csv(args.out, index=False)
    print(f"Saved synthetic data to {args.out} with shape {df.shape}")

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import joblib

class ScalerWrapper:
    def __init__(self):
        self.scaler_x = StandardScaler()
        self.scaler_y = StandardScaler()
    def fit(self, X, y):
        # X: (T, n_features), y: (T,1)
        self.scaler_x.fit(X)
        self.scaler_y.fit(y.reshape(-1,1))
    def transform(self, X, y=None):
        Xs = self.scaler_x.transform(X)
        if y is None:
            return Xs
        ys = self.scaler_y.transform(y.reshape(-1,1)).reshape(-1)
        return Xs, ys
    def inverse_y(self, y_scaled):
        return self.scaler_y.inverse_transform(y_scaled.reshape(-1,1)).reshape(-1)

def create_windows(X, y, input_len=48, horizon=12):
    """
    X: (T, n_features)
    y: (T,)
    returns list of (x_window, y_target) where:
    x_window: (input_len, n_features)
    y_target: scalar or (horizon,)
    """
    T = len(y)
    windows = []
    for end in range(input_len, T - horizon + 1):
        start = end - input_len
        x = X[start:end]
        y_target = y[end:end+horizon]
        windows.append((x.astype(np.float32), y_target.astype(np.float32)))
    return windows

def rolling_origin_splits(df_len, initial_train_size, horizon, step):
    """
    yields (train_end, val_end) indices (end-exclusive)
    - train: [0:train_end)
    - val(test): [train_end:val_end)
    """
    train_end = initial_train_size
    while train_end + horizon <= df_len:
        val_end = train_end + horizon
        yield train_end, val_end
        train_end += step

def load_csv(path):
    df = pd.read_csv(path)
    return df

"""
PyTorch models: Baseline LSTM, LSTM+Bahdanau attention, Transformer encoder.
"""
import torch
import torch.nn as nn
import math

class LSTMBase(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, n_layers=2, dropout=0.1):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)
        self.out = nn.Linear(hidden_dim, 1)
    def forward(self, x):
        # x: (B, seq_len, input_dim)
        out, (h, c) = self.lstm(x)
        # use last hidden state
        last = out[:, -1, :]
        y = self.out(last).squeeze(-1)
        return y

class BahdanauAttention(nn.Module):
    def __init__(self, hidden_dim, attn_dim):
        super().__init__()
        self.W1 = nn.Linear(hidden_dim, attn_dim, bias=False)
        self.W2 = nn.Linear(hidden_dim, attn_dim, bias=False)
        self.V = nn.Linear(attn_dim, 1, bias=False)
    def forward(self, encoder_outputs, decoder_hidden):
        # encoder_outputs: (B, seq_len, hidden_dim)
        # decoder_hidden: (B, hidden_dim)  (we'll reuse last hidden)
        seq_len = encoder_outputs.size(1)
        # expand decoder hidden to seq_len
        dec_exp = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)
        score = torch.tanh(self.W1(encoder_outputs) + self.W2(dec_exp))
        attn_weights = torch.softmax(self.V(score).squeeze(-1), dim=-1)  # (B, seq_len)
        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (B, hidden_dim)
        return context, attn_weights

class AttentiveLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, n_layers=1, attn_dim=32, dropout=0.1):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)
        self.attn = BahdanauAttention(hidden_dim, attn_dim)
        self.out = nn.Linear(hidden_dim, 1)
    def forward(self, x):
        enc_out, (h, c) = self.lstm(x)  # enc_out: (B, seq_len, hidden)
        last_hidden = enc_out[:, -1, :]  # (B, hidden)
        context, attn_weights = self.attn(enc_out, last_hidden)
        y = self.out(context).squeeze(-1)
        return y, attn_weights

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

class TransformerEncoderModel(nn.Module):
    def __init__(self, input_dim, d_model=64, n_heads=4, n_layers=3, dim_feedforward=128, dropout=0.1):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, d_model)
        self.pos_enc = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads, dim_feedforward, dropout, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        self.out = nn.Linear(d_model, 1)
    def forward(self, x):
        # x: (B, seq_len, input_dim)
        h = self.input_proj(x)
        h = self.pos_enc(h)
        enc_out = self.encoder(h)  # (B, seq_len, d_model)
        last = enc_out[:, -1, :]
        y = self.out(last).squeeze(-1)
        return y

"""
Train script that:
- loads CSV
- scales
- creates rolling-origin splits
- trains specified model(s)
- saves models, scalers, training logs, attention weights (if applicable)
"""
import argparse
import os
import numpy as np
import pandas as pd
from data_utils import ScalerWrapper, create_windows, rolling_origin_splits, load_csv
from models import LSTMBase, AttentiveLSTM, TransformerEncoderModel
import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
from tqdm import tqdm
import joblib

def prepare_data(df, input_len=48, horizon=1):
    features = [c for c in df.columns if c not in ['t','y']]
    X = df[features].values
    y = df['y'].values
    scaler = ScalerWrapper()
    scaler.fit(X, y)
    Xs, ys = scaler.transform(X, y)
    windows = create_windows(Xs, ys, input_len=input_len, horizon=horizon)
    return windows, scaler, features

def windows_to_dataset(windows):
    xs = np.stack([w[0] for w in windows])  # (N, seq, feat)
    ys = np.stack([w[1] for w in windows])  # (N, horizon)
    # for horizon==1 flatten
    if ys.shape[1] == 1:
        ys = ys[:,0]
    return xs, ys

def train_model(model, train_loader, val_loader, device, n_epochs=20, lr=1e-3):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    model.to(device)
    best_val = 1e9
    history = {'train_loss':[], 'val_loss':[]}
    for epoch in range(n_epochs):
        model.train()
        running = 0.0
        for xb, yb in train_loader:
            xb = xb.to(device)
            yb = yb.to(device)
            optimizer.zero_grad()
            out = model(xb)
            if isinstance(out, tuple):
                out = out[0]
            loss = criterion(out, yb)
            loss.backward()
            optimizer.step()
            running += loss.item() * xb.size(0)
        train_loss = running / len(train_loader.dataset)
        # val
        model.eval()
        vrunning = 0.0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                yb = yb.to(device)
                out = model(xb)
                if isinstance(out, tuple):
                    out = out[0]
                loss = criterion(out, yb)
                vrunning += loss.item() * xb.size(0)
        val_loss = vrunning / len(val_loader.dataset)
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        # simple checkpoint
        if val_loss < best_val:
            best_val = val_loss
            best_state = model.state_dict()
        print(f"Epoch {epoch+1}/{n_epochs} train_loss={train_loss:.6f} val_loss={val_loss:.6f}")
    model.load_state_dict(best_state)
    return model, history

def run(args):
    os.makedirs(args.out_dir, exist_ok=True)
    df = load_csv(args.data)
    # prepare full scaled windows (we will split by index later)
    input_len = args.input_len
    horizon = args.horizon
    windows, scaler, features = prepare_data(df, input_len=input_len, horizon=horizon)
    # Convert windows back to a sequence index: windows correspond to end indices from input_len..T-horizon
    T = len(df)
    start_idx = input_len
    end_idx = T - horizon
    # We'll perform rolling origin splits on raw time index using initial_train_size
    initial_train = args.initial_train
    step = args.step
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    split_id = 0
    all_histories = []
    # convert windows list to arrays of same order as time
    xs, ys = np.stack([w[0] for w in windows]), np.stack([w[1] for w in windows])
    if ys.ndim == 2 and ys.shape[1] == 1:
        ys = ys[:,0]
    # mapping: windows[i] corresponds to end index = start_idx + i
    for train_end, val_end in rolling_origin_splits(T, initial_train, horizon, step):
        # map to window indices: valid window end indices are start_idx..end_idx-1 inclusive
        win_train_end = train_end - start_idx
        win_val_end = val_end - start_idx
        if win_train_end <= 0:
            continue
        X_train = xs[:win_train_end]
        y_train = ys[:win_train_end]
        X_val = xs[win_train_end:win_val_end]
        y_val = ys[win_train_end:win_val_end]
        # build loaders
        batch_size = args.batch_size
        train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))
        val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))
        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

        # instantiate model
        input_dim = X_train.shape[2]
        if args.model == 'lstm':
            model = LSTMBase(input_dim, hidden_dim=args.hidden)
        elif args.model == 'attentive_lstm':
            model = AttentiveLSTM(input_dim, hidden_dim=args.hidden)
        elif args.model == 'transformer':
            model = TransformerEncoderModel(input_dim, d_model=args.hidden)
        else:
            raise ValueError("Unknown model")
        print(f"Split {split_id}: training {args.model} with {len(train_ds)} train samples, {len(val_ds)} val samples")
        model, history = train_model(model, train_loader, val_loader, device, n_epochs=args.epochs, lr=args.lr)
        # Save model and scaler for this fold
        split_dir = os.path.join(args.out_dir, f"{args.model}_split{split_id}")
        os.makedirs(split_dir, exist_ok=True)
        torch.save(model.state_dict(), os.path.join(split_dir, "model.pth"))
        joblib.dump(scaler, os.path.join(split_dir, "scaler.joblib"))
        # Save training history
        pd.DataFrame(history).to_csv(os.path.join(split_dir, "history.csv"), index=False)
        split_id += 1
        all_histories.append(history)
        if split_id >= args.max_splits:
            break
    print("Training complete. Saved models in:", args.out_dir)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', required=True)
    parser.add_argument('--out_dir', default='experiments/results')
    parser.add_argument('--model', choices=['lstm','attentive_lstm','transformer'], default='attentive_lstm')
    parser.add_argument('--input_len', type=int, default=48)
    parser.add_argument('--horizon', type=int, default=1)
    parser.add_argument('--initial_train', type=int, default=800)
    parser.add_argument('--step', type=int, default=24)
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--hidden', type=int, default=64)
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--max_splits', type=int, default=5)
    args = parser.parse_args()
    run(args)

"""
Evaluate saved models and SARIMA baseline, compute RMSE/MAE/MAPE
Plot forecasts, attention weights if available.
"""
import argparse
import os
import numpy as np
import pandas as pd
import torch
from data_utils import load_csv, ScalerWrapper, create_windows
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import joblib

def rmse(a,b): return math.sqrt(mean_squared_error(a,b))
def mape(a,b): return np.mean(np.abs((a-b)/ (np.where(np.abs(a) < 1e-6, 1e-6, a)))) * 100.0

def evaluate_model_on_test(df, model_dir, input_len=48, horizon=1):
    scaler = joblib.load(os.path.join(model_dir, "scaler.joblib"))
    model_state = os.path.join(model_dir, "model.pth")
    # detect model type from folder name
    model_name = os.path.basename(model_dir).split('_')[0]
    from models import LSTMBase, AttentiveLSTM, TransformerEncoderModel
    features = [c for c in df.columns if c not in ['t','y']]
    X = df[features].values
    y = df['y'].values
    Xs = scaler.scaler_x.transform(X)
    ys = scaler.scaler_y.transform(y.reshape(-1,1)).reshape(-1)
    windows = create_windows(Xs, ys, input_len=input_len, horizon=horizon)
    xs, ys_w = zip(*windows)
    xs = np.stack(xs)
    ys_w = np.stack(ys_w)
    if ys_w.ndim==2 and ys_w.shape[1]==1:
        ys_w = ys_w[:,0]
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    input_dim = xs.shape[2]
    if model_name == 'lstm':
        model = LSTMBase(input_dim)
    elif model_name == 'attentive':
        model = AttentiveLSTM(input_dim)
    elif model_name == 'attentive_lstm':
        model = AttentiveLSTM(input_dim)
    elif model_name == 'transformer':
        model = TransformerEncoderModel(input_dim)
    else:
        # fallback
        model = LSTMBase(input_dim)
    model.load_state_dict(torch.load(model_state, map_location=device))
    model.to(device).eval()
    batch = torch.from_numpy(xs.astype(np.float32))
    with torch.no_grad():
        out = model(batch.to(device))
    if isinstance(out, tuple):
        preds = out[0].cpu().numpy()
        attn = out[1].cpu().numpy()
    else:
        preds = out.cpu().numpy()
        attn = None
    # invert scale
    preds_inv = scaler.inverse_y(preds)
    ys_inv = scaler.inverse_y(ys_w)
    metrics = {
        'RMSE': rmse(ys_inv, preds_inv),
        'MAE': mean_absolute_error(ys_inv, preds_inv),
        'MAPE': mape(ys_inv, preds_inv),
    }
    return preds_inv, ys_inv, attn, metrics

def sarima_forecast(df, order=(1,1,1), seasonal_order=(0,0,0,0), train_end=1000, horizon=1, feature_col='y'):
    # Univariate SARIMA on target series
    series = df[feature_col].values
    train = series[:train_end]
    model = SARIMAX(train, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)
    res = model.fit(disp=False)
    start = train_end
    end = train_end + horizon - 1
    forecast = res.predict(start=start, end=end)
    return forecast

def run(args):
    os.makedirs(args.out_dir, exist_ok=True)
    df = load_csv(args.data)
    results = {}
    # evaluate each saved split folder
    for fld in sorted(os.listdir(args.results_dir)):
        fldpath = os.path.join(args.results_dir, fld)
        if not os.path.isdir(fldpath): continue
        if not os.path.exists(os.path.join(fldpath,'model.pth')): continue
        preds, ys, attn, metrics = evaluate_model_on_test(df, fldpath, input_len=args.input_len, horizon=args.horizon)
        results[fld] = {'metrics':metrics, 'preds':preds, 'ys':ys, 'attn':attn}
        # save a plot comparing first 200 points
        plt.figure()
        nplot = min(200, len(ys))
        plt.plot(ys[:nplot], label='actual')
        plt.plot(preds[:nplot], label='pred')
        plt.legend()
        plt.title(f"{fld} forecast vs actual")
        plt.tight_layout()
        plt.savefig(os.path.join(args.out_dir, f"{fld}_forecast.png"))
        plt.close()
        if attn is not None:
            # average attention across batch to visualize
            mean_attn = attn.mean(axis=0)
            plt.figure(figsize=(6,3))
            plt.imshow(mean_attn[np.newaxis,:], aspect='auto')
            plt.title(f"{fld} mean attention")
            plt.xlabel('time-step')
            plt.yticks([])
            plt.colorbar()
            plt.tight_layout()
            plt.savefig(os.path.join(args.out_dir, f"{fld}_attn.png"))
            plt.close()
    # SARIMA baseline: fit on first N and forecast next horizon for each split start
    # We'll compute a simple SARIMA forecast for a single split example
    sarima_metrics = {}
    # default config: compute single SARIMA for a few train_end points
    for train_end in [800, 1000, 1200]:
        try:
            f = sarima_forecast(df, train_end=train_end, horizon=args.horizon)
            actual = df['y'].values[train_end:train_end+args.horizon]
            sarima_metrics[f'sarima_trainend_{train_end}'] = {
                'RMSE': rmse(actual, f),
                'MAE': mean_absolute_error(actual, f),
                'MAPE': mape(actual, f)
            }
        except Exception as e:
            sarima_metrics[f'sarima_trainend_{train_end}'] = {'error':str(e)}
    # Summarize results
    summary = {}
    for k,v in results.items():
        summary[k] = v['metrics']
    summary['sarima'] = sarima_metrics
    pd.DataFrame(summary).T.to_csv(os.path.join(args.out_dir, "summary_metrics.csv"))
    print("Saved evaluation outputs to", args.out_dir)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', required=True)
    parser.add_argument('--results_dir', default='experiments/results')
    parser.add_argument('--out_dir', default='experiments/results/eval')
    parser.add_argument('--input_len', type=int, default=48)
    parser.add_argument('--horizon', type=int, default=1)
    args = parser.parse_args()
    run(args)

"""
Produce a plain-text technical report that summarizes methodology, hyperparameters, and the results.
"""
import argparse
import os
import pandas as pd



def run(args):
    df = pd.read_csv(args.data)
    n_steps = len(df)
    n_series = len([c for c in df.columns if c not in ['t','y']])
    text = TEMPLATE.format(
        n_series=n_series, n_steps=n_steps, data_path=args.data,
        initial_train=args.initial_train, horizon=args.horizon, step=args.step, max_splits=args.max_splits,
        input_len=args.input_len, hidden=args.hidden, batch_size=args.batch_size, epochs=args.epochs, lr=args.lr
    )
    with open(args.out, 'w') as f:
        f.write(text)
    print("Saved technical report to", args.out)

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', required=True)
    parser.add_argument('--out', default='technical_report.txt')
    parser.add_argument('--initial_train', type=int, default=800)
    parser.add_argument('--horizon', type=int, default=1)
    parser.add_argument('--step', type=int, default=24)
    parser.add_argument('--max_splits', type=int, default=5)
    parser.add_argument('--input_len', type=int, default=48)
    parser.add_argument('--hidden', type=int, default=64)
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--lr', type=float, default=1e-3)
    args = parser.parse_args()
    run(args)

